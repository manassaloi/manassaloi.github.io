---
layout: post
title:  "The OpenEvidence playbook"
---

OpenEvidence acquired over 350,000 doctors within two years by solving a fundamental issue: medical knowledge is expanding faster than any human can keep up with. Two new medical papers appear every minute, doubling the corpus of medical research every 73 days.

It’s humanly impossible for doctors to read everything relevant.

Instead of training a model on the entire public internet, OpenEvidence focused on smaller, specialised models trained exclusively on a trusted corpus: over 35 million peer-reviewed medical publications, with exclusive access to top-tier journals like The New England Journal of Medicine.

It’s not some generalist chatbot like ChatGPT, it's a decision support tool that gives doctors evidence backed answers. It can instantly search, summarise, and extract the most critical insights doctors need right at the point of care.

But this overload problem isn’t unique to medicine.

Professionals in multiple industries are drowning under the same firehose of information. For investors, lawyers, cybersecurity experts, and researchers in cutting edge fields, the risks of missing key information can be equally severe.

Here’s what makes an industry ripe for an “OpenEvidence-for-X” product:
- The amount of information professionals must handle is overwhelming and growing exponentially.
- The cost of missing something crucial is high enough that people will happily pay to avoid it.
- The content they rely on has clear, authoritative sources like medical journals for doctors or SEC filings for financial analysts.
- Professionals need fast, actionable answers embedded directly into their existing workflows, rather than standalone tools.

A few industries that ready for their own OpenEvidence:
- Legal professionals constantly sift through millions of court rulings and regulations. Just missing one new precedent could tank a case. Companies like Harvey.ai and Lexis+ AI have emerged to tackle this.
- Finance analysts face a relentless barrage: over two million Reuters stories a year, thousands of earnings calls, SEC filings dropping constantly. Time is money in trading.
- Cybersecurity teams get hit with over 100 new software vulnerabilities per day. Delaying just one security patch might lead to catastrophic breaches.
- There’s the tsunami of scientific research, especially in rapidly evolving fields like AI, climate tech, and materials science. Every week, thousands of new papers flood databases like arXiv.
- Patents and regulatory documents are similarly daunting. Over 3 million patents were filed worldwide last year alone. A company missing a critical patent could face costly infringement lawsuits.
- Regulatory professionals overwhelmed by thousands of pages of new rules each month would greatly benefit from a tool that translates complex legislation into actionable briefs and compliance checklists, saving countless hours.

These industries share clear design patterns:
- They rely on authoritative sources: court documents from PACER, SEC filings from EDGAR, vulnerabilities from NVD databases.
- Regulatory or professional duty to “stay current.”
- They require pinpoint granular semantic indexing, providing exactly the right information down to the paragraph.
- Every recommendation must link back explicitly to original sources to maintain trust and compliance.
- Solutions can integrate seamlessly into users’ daily tools.

AI startups that nail these details see rapid adoption, and users willingly pay for reliability and precision. The market is already validating this need. BloombergGPT is embedding deep semantic searches directly into financial workflows. Harvey.ai is embedding itself in legal processes. GitHub Dependabot is automating software security upgrades.

Each solves real problems with specialised AI, similiar to what OpenEvidence did in medicine.

Any industry overwhelmed by growing complexity, facing its own information overload crisis, where the consequences of a single missed detail can be severe, is primed for disruption by its own version of OpenEvidence.

The best vertical AI software will win by controlling data that the horizontal LLM foundational model giants can’t touch, and when the field punishes outdated information.

If you are scouting for vertical AI opportunities try to nail proprietary hard to find data that you can lock down through exclusive license, focus on retrieval quality and the domain specific UX by seamlessly integrating into your users’ daily workflow.
